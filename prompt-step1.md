LLMのファインチューニングで文書分類タスクを行い、そのモデルを蒸留することでより小さなリソースで実行できるようにします。まずはファインチューニングです。以下の条件を含めてコードを作成してください。

## ファインチューニング
- M1チップのmacOS上で実行するものとします。cudaを指定しないでください。
- モデルはcl-tohoku/bert-base-japanese-v3を使うこと。
  - モデルのパラメータ数を出力してください。
- データはlivedoorニュースコーパス（ldcc-20140209.tar.gz）を使うこと。すでにダウンロードしており、以下のように配置済みとします。
  - ``~/data/livedoor-text/カテゴリ名/*.txt``
  - ``~/`` はホームディレクトリのことです。絶対パスに変換してから利用してください。
  - ``~/data/livedoor-text/`` 直下にはディレクトリ以外のファイルも含まれています。ディレクトリであるか判定し、ディレクトリである場合にのみデータとして読み込んでください。
  - カテゴリ名が分類ラベルに相当します。dokujo-tsushin, it-life-hack, kaden-channel, livedoor-homme, movie-enter, peachy, smax, sports-watch, topic-newsの9ラベルがあります。このうち dokujo-tsushin, it-life-hack の2つだけを利用してください。
  - 上記テキストを読み込み、カテゴリ名を辞書順にソートしてラベルを割り振り、文書を読み込んでください。例えば dokujo-tsushin は辞書順で最初になるのでラベル0です。このディレクトリの中にある全てのテキストの正解ラベルを0とします。同様に it-life-hack は辞書順で2番目になるためラベル1とし、このディレクトリの中にある全てのテキストの正解ラベルを1とします。それ以外のディレクトリは無視してください。
  - テキストファイルの冒頭2行目は削除してください。
- データセットの構築
  - train:val:test=6:2:2 の割合でデータを分割してください。この際、シード値=1でシャッフルしておいてください。
- ファインチューニング
  - 代表的なパラメータを設定してください。
  - 最大エポック数は20とします。
  - EarlyStoppingによりvalの損失をモニタリングしつつ、3回改善されなかった場合には学習を停止してください。このために ``from transformers import EarlyStoppingCallback`` を利用してください。
  - train, valに対するエポック毎の損失推移を描画してください。
  - 最終モデルを保存し、train, val, testそれぞれに対する分類精度を出力してください。またそれぞれの推論に要した実行時間を出力してください。
