{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v3 were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-v3 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of model parameters: 111208706\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from transformers import BertJapaneseTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "\n",
    "# モデルとトークナイザの準備\n",
    "model_name = \"cl-tohoku/bert-base-japanese-v3\"\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# モデルのパラメータ数を出力\n",
    "model_parameters = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Number of model parameters: {model_parameters}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_dataset)=1045\n",
      "train_texts[0]='結婚しても働くのはなぜ？ 既婚女性のつぶやき\\n\\u3000「彼の収入が少ないから私も働かなければならないし、それを思うと結婚はもう少し先でもいいかな」と結婚を躊躇する独女がいる。彼女は彼の収入だけで暮らせるのなら、仕事は今すぐにでも辞めたいらしい。つまり専業主婦志望なのだが、彼の年収を聞いて首を傾げた。\\n\\n\\u3000この金額で本当に生活ができないのだろうか？ \\n\\n\\u3000かつて専業主婦が多かった時代、主婦の働き先はなく、今月もかつかつだとこぼしながらも、夫の稼ぎだけで暮らしていた家庭が多かった。しかし今は不況で夫の収入が減ったとはいえ、外食、ブランド品購入、安いツアーとはいえ海外旅行にも行っている。食べるだけで精一杯の昔に比べれば、ものすごく贅沢ではないだろうか？\\n\\u3000\\n\\u3000成人した二人の子供がいる専業主婦の紀世子さん（56 歳）は、「今は専業主婦がセレブのように言われますけど、私はブランド品も持ったことがなければ、家族で海外旅行にも行ったことがないんですよ。夫の収入だけで充分とはいいませんけど、贅沢さえしなければ毎月何とかなったものです」という。\\n\\u3000\\n\\u3000子供が小学校に入学すると、塾の費用を捻出するためにパートに行く主婦もいたが、紀世子さんの家庭はご主人の方針で塾には一切通わせず、兄は水泳、妹は習字と、週に一度の習い事に通わせただけだそうだ。\\n\\n\\u3000「私立中学受験で塾に通わせているご家庭は大変そうでしたよ。塾の費用が一か月5万円と聞いてびっくりしました。そこまでして私立に行かせて、その後も莫大な教育費がかかるのに大変だとあと思いました」\\n\\n\\u3000紀世子さんの長女は私立の女子大学に入学したが、中学・高校から持ち上がりできた友人には小学校の時の同級生もいる。「中高一貫教育の必要性はよく分かりませんが、結局同じ大学に通うなら何も高い教育費を払って中学から行く必要がないのでは？」これは私の考えですがと紀世子さん。\\n\\n\\u3000仕事に生きがいを持ち自分のために働いている主婦もいるが、家族で海外旅行に行ったり外食をしたり、生活水準を上げるために働いている主婦もいる。自分の稼ぎでブランド品を買う主婦もいるが、やはり主婦の働く目的の大半は子供の教育費の捻出だろう。\\n\\n\\u3000教育費は、子どもが生まれてから大学卒業まで一般に1000万円以上もかかると言われている。幼稚園から大学まで私立なら2000万円は超す。となれば教育費のために働かなければならないわけだが、幼稚園から私立にやるのはどうしてなのだろうか？\\n\\n\\u3000「子供に誇れる学歴をつけてやりたいからです」\\n\\n\\u3000私立幼稚園入学を目指している幼児の母親、A子さんはいうが、誇れるとはすなわち母親の価値観で、他の子と比べて自分の子供は特別なことをさせているという母親自身の見栄もあるのかもしれない。\\n子供を私立幼稚園に行かせたり、ブランドの服を着せたりすれば、母親もそれに見合う服装やバックを身につけなければならなくなる。\\n\\u3000\\n\\u3000そういう生活がしたいけれど、夫の収入でできなければ、我慢すればいい。我慢できなければ働けばいい。けれど、働く目的が子供のためであるなら、なにが本当に子供にとって幸せなのかを考えるべきではないだろうか。\\n\\n\\u3000彼の収入が少ないとか夫の稼ぎが足りないとこぼす女性たちの胸の内は、なくてもいいものをあえて欲しがる暮らしを求めている気がする。\\n人と競い合うことで向上することもあるが、家庭における幸せとは決して比べたり競い合うものではないと思う。\\n\\n\\u3000前述の紀世子さんだが、学校から帰ってくるといつも「お帰りなさい」と待っていてくれるお母さんがいてくれて嬉しかったと成人した娘さんから言われたそうだ。\\n「能力も資格もないので家にいて節約しているだけの生活でしたが、子供と過ごせた時間は楽しかったですよ」\\n\\u3000\\n\\u3000養ってくれた夫にも感謝しているという紀世子さんの言葉がものすごく新鮮に聞こえた。\\n\\n\\u3000働いて得るものもあるが、節約して作った時間で得るものもある。彼の収入が少ないと思うのなら、やりくりという算段を覚えることをしてみてはどうだろうか？ 結婚はいろいろ頭で考えているより、実際生活をしてみればなるようになるものです。（オフィスエムツー／佐枝せつこ）'\n",
      "train_labels[0]=0\n"
     ]
    }
   ],
   "source": [
    "# データの読み込みと前処理\n",
    "def load_data(directory):\n",
    "    texts, labels = [], []\n",
    "    for label, category in enumerate([\"dokujo-tsushin\", \"it-life-hack\"]):\n",
    "        category_dir = os.path.join(directory, category)\n",
    "        for file in os.listdir(category_dir):\n",
    "            if os.path.isfile(os.path.join(category_dir, file)):\n",
    "                with open(os.path.join(category_dir, file), 'r', encoding='utf-8') as f:\n",
    "                    lines = f.readlines()[2:]  # 最初の2行をスキップ\n",
    "                    text = ''.join(lines).strip()\n",
    "                    texts.append(text)\n",
    "                    labels.append(label)\n",
    "    return texts, labels\n",
    "\n",
    "home_dir = os.path.expanduser(\"~\")\n",
    "data_dir = os.path.join(home_dir, \"data/livedoor-text\")\n",
    "texts, labels = load_data(data_dir)\n",
    "\n",
    "# データセットの分割\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.4, random_state=1)\n",
    "val_texts, test_texts, val_labels, test_labels = train_test_split(test_texts, test_labels, test_size=0.5, random_state=1)\n",
    "\n",
    "# データセットの準備\n",
    "class LivedoorDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=512)\n",
    "\n",
    "train_dataset = LivedoorDataset(train_encodings, train_labels)\n",
    "val_dataset = LivedoorDataset(val_encodings, val_labels)\n",
    "test_dataset = LivedoorDataset(test_encodings, test_labels)\n",
    "\n",
    "print(f\"{len(train_dataset)=}\")\n",
    "print(f\"{train_texts[0]=}\")\n",
    "print(f\"{train_labels[0]=}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tnal/.venv/opencalm/lib/python3.9/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac0ef11ce6034bafa580d42358229e32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2620 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6939, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.08}\n",
      "{'loss': 0.6485, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.15}\n",
      "{'loss': 0.6016, 'learning_rate': 3e-06, 'epoch': 0.23}\n",
      "{'loss': 0.4639, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.31}\n",
      "{'loss': 0.4111, 'learning_rate': 5e-06, 'epoch': 0.38}\n",
      "{'loss': 0.2809, 'learning_rate': 6e-06, 'epoch': 0.46}\n",
      "{'loss': 0.2223, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.53}\n",
      "{'loss': 0.1178, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.61}\n",
      "{'loss': 0.0928, 'learning_rate': 9e-06, 'epoch': 0.69}\n",
      "{'loss': 0.0277, 'learning_rate': 1e-05, 'epoch': 0.76}\n",
      "{'loss': 0.0405, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.84}\n",
      "{'loss': 0.0107, 'learning_rate': 1.2e-05, 'epoch': 0.92}\n",
      "{'loss': 0.0666, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6c2e211811348d3b8c1df9dfcd4085e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.12046581506729126, 'eval_runtime': 97.0742, 'eval_samples_per_second': 3.585, 'eval_steps_per_second': 0.453, 'epoch': 1.0}\n",
      "{'loss': 0.0047, 'learning_rate': 1.4000000000000001e-05, 'epoch': 1.07}\n",
      "{'loss': 0.0096, 'learning_rate': 1.5e-05, 'epoch': 1.15}\n",
      "{'loss': 0.055, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.22}\n",
      "{'loss': 0.1524, 'learning_rate': 1.7000000000000003e-05, 'epoch': 1.3}\n",
      "{'loss': 0.0024, 'learning_rate': 1.8e-05, 'epoch': 1.37}\n",
      "{'loss': 0.0243, 'learning_rate': 1.9e-05, 'epoch': 1.45}\n",
      "{'loss': 0.0058, 'learning_rate': 2e-05, 'epoch': 1.53}\n",
      "{'loss': 0.0031, 'learning_rate': 2.1e-05, 'epoch': 1.6}\n",
      "{'loss': 0.0008, 'learning_rate': 2.2000000000000003e-05, 'epoch': 1.68}\n",
      "{'loss': 0.0606, 'learning_rate': 2.3000000000000003e-05, 'epoch': 1.76}\n",
      "{'loss': 0.0009, 'learning_rate': 2.4e-05, 'epoch': 1.83}\n",
      "{'loss': 0.0035, 'learning_rate': 2.5e-05, 'epoch': 1.91}\n",
      "{'loss': 0.0117, 'learning_rate': 2.6000000000000002e-05, 'epoch': 1.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aac88c9842147dba61a8366c7747c10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.046354349702596664, 'eval_runtime': 96.3035, 'eval_samples_per_second': 3.614, 'eval_steps_per_second': 0.457, 'epoch': 2.0}\n",
      "{'loss': 0.001, 'learning_rate': 2.7000000000000002e-05, 'epoch': 2.06}\n",
      "{'loss': 0.0005, 'learning_rate': 2.8000000000000003e-05, 'epoch': 2.14}\n",
      "{'loss': 0.0004, 'learning_rate': 2.9e-05, 'epoch': 2.21}\n",
      "{'loss': 0.0003, 'learning_rate': 3e-05, 'epoch': 2.29}\n",
      "{'loss': 0.0004, 'learning_rate': 3.1e-05, 'epoch': 2.37}\n",
      "{'loss': 0.0003, 'learning_rate': 3.2000000000000005e-05, 'epoch': 2.44}\n",
      "{'loss': 0.0003, 'learning_rate': 3.3e-05, 'epoch': 2.52}\n",
      "{'loss': 0.0002, 'learning_rate': 3.4000000000000007e-05, 'epoch': 2.6}\n",
      "{'loss': 0.0002, 'learning_rate': 3.5e-05, 'epoch': 2.67}\n",
      "{'loss': 0.0002, 'learning_rate': 3.6e-05, 'epoch': 2.75}\n",
      "{'loss': 0.0046, 'learning_rate': 3.7e-05, 'epoch': 2.82}\n",
      "{'loss': 0.0003, 'learning_rate': 3.8e-05, 'epoch': 2.9}\n",
      "{'loss': 0.0541, 'learning_rate': 3.9000000000000006e-05, 'epoch': 2.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6533bfe9233a44c3808a7a34abbdd337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.04289337247610092, 'eval_runtime': 96.4297, 'eval_samples_per_second': 3.609, 'eval_steps_per_second': 0.456, 'epoch': 3.0}\n",
      "{'loss': 0.0044, 'learning_rate': 4e-05, 'epoch': 3.05}\n",
      "{'loss': 0.0004, 'learning_rate': 4.1e-05, 'epoch': 3.13}\n",
      "{'loss': 0.0674, 'learning_rate': 4.2e-05, 'epoch': 3.21}\n",
      "{'loss': 0.0084, 'learning_rate': 4.3e-05, 'epoch': 3.28}\n",
      "{'loss': 0.0579, 'learning_rate': 4.4000000000000006e-05, 'epoch': 3.36}\n",
      "{'loss': 0.004, 'learning_rate': 4.5e-05, 'epoch': 3.44}\n",
      "{'loss': 0.0075, 'learning_rate': 4.600000000000001e-05, 'epoch': 3.51}\n",
      "{'loss': 0.0003, 'learning_rate': 4.7e-05, 'epoch': 3.59}\n",
      "{'loss': 0.0692, 'learning_rate': 4.8e-05, 'epoch': 3.66}\n",
      "{'loss': 0.0539, 'learning_rate': 4.9e-05, 'epoch': 3.74}\n",
      "{'loss': 0.0027, 'learning_rate': 5e-05, 'epoch': 3.82}\n",
      "{'loss': 0.0001, 'learning_rate': 4.976415094339622e-05, 'epoch': 3.89}\n",
      "{'loss': 0.0001, 'learning_rate': 4.952830188679246e-05, 'epoch': 3.97}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e0388a3b3f64580bc1a9da9d46874fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.07540009170770645, 'eval_runtime': 94.5978, 'eval_samples_per_second': 3.679, 'eval_steps_per_second': 0.465, 'epoch': 4.0}\n",
      "{'loss': 0.0003, 'learning_rate': 4.929245283018868e-05, 'epoch': 4.05}\n",
      "{'loss': 0.0001, 'learning_rate': 4.9056603773584906e-05, 'epoch': 4.12}\n",
      "{'loss': 0.0001, 'learning_rate': 4.8820754716981134e-05, 'epoch': 4.2}\n",
      "{'loss': 0.0001, 'learning_rate': 4.858490566037736e-05, 'epoch': 4.27}\n",
      "{'loss': 0.0001, 'learning_rate': 4.834905660377358e-05, 'epoch': 4.35}\n",
      "{'loss': 0.0001, 'learning_rate': 4.811320754716982e-05, 'epoch': 4.43}\n",
      "{'loss': 0.0001, 'learning_rate': 4.787735849056604e-05, 'epoch': 4.5}\n",
      "{'loss': 0.0001, 'learning_rate': 4.7641509433962266e-05, 'epoch': 4.58}\n",
      "{'loss': 0.0, 'learning_rate': 4.7405660377358494e-05, 'epoch': 4.66}\n",
      "{'loss': 0.0001, 'learning_rate': 4.716981132075472e-05, 'epoch': 4.73}\n",
      "{'loss': 0.0, 'learning_rate': 4.693396226415094e-05, 'epoch': 4.81}\n",
      "{'loss': 0.0, 'learning_rate': 4.669811320754717e-05, 'epoch': 4.89}\n",
      "{'loss': 0.0006, 'learning_rate': 4.64622641509434e-05, 'epoch': 4.96}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f12be575154d40a9949bdad2d98f0415",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.08540798723697662, 'eval_runtime': 96.1918, 'eval_samples_per_second': 3.618, 'eval_steps_per_second': 0.457, 'epoch': 5.0}\n",
      "{'loss': 0.0, 'learning_rate': 4.6226415094339625e-05, 'epoch': 5.04}\n",
      "{'loss': 0.0, 'learning_rate': 4.5990566037735846e-05, 'epoch': 5.11}\n",
      "{'loss': 0.0, 'learning_rate': 4.575471698113208e-05, 'epoch': 5.19}\n",
      "{'loss': 0.0, 'learning_rate': 4.55188679245283e-05, 'epoch': 5.27}\n",
      "{'loss': 0.0, 'learning_rate': 4.528301886792453e-05, 'epoch': 5.34}\n",
      "{'loss': 0.0, 'learning_rate': 4.504716981132076e-05, 'epoch': 5.42}\n",
      "{'loss': 0.0, 'learning_rate': 4.4811320754716985e-05, 'epoch': 5.5}\n",
      "{'loss': 0.0, 'learning_rate': 4.4575471698113206e-05, 'epoch': 5.57}\n",
      "{'loss': 0.0, 'learning_rate': 4.433962264150944e-05, 'epoch': 5.65}\n",
      "{'loss': 0.0002, 'learning_rate': 4.410377358490566e-05, 'epoch': 5.73}\n",
      "{'loss': 0.0, 'learning_rate': 4.386792452830189e-05, 'epoch': 5.8}\n",
      "{'loss': 0.0, 'learning_rate': 4.363207547169812e-05, 'epoch': 5.88}\n",
      "{'loss': 0.0, 'learning_rate': 4.3396226415094345e-05, 'epoch': 5.95}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9edbdfd588e4a988a6bce63362fa808",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.08984164893627167, 'eval_runtime': 96.447, 'eval_samples_per_second': 3.608, 'eval_steps_per_second': 0.456, 'epoch': 6.0}\n",
      "{'train_runtime': 8791.3103, 'train_samples_per_second': 2.377, 'train_steps_per_second': 0.298, 'train_loss': 0.055398016048288125, 'epoch': 6.0}\n",
      "Train Accuracy: 1.0, Time: 290.0567800998688\n",
      "Validation Accuracy: 0.9885057471264368, Time: 96.65163064002991\n",
      "Test Accuracy: 0.9885386819484241, Time: 97.23863410949707\n"
     ]
    }
   ],
   "source": [
    "# トレーニングの設定\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=20,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "# トレーニングの実行\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# モデルの保存\n",
    "model.save_pretrained(\"./finetuned_bert_japanese\")\n",
    "\n",
    "# 評価\n",
    "def evaluate_model(model, dataset, batch_size=8):\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "    model.eval()\n",
    "    total, correct = 0, 0\n",
    "    start_time = time.time()\n",
    "    for batch in dataloader:\n",
    "        inputs = {k: v.to(model.device) for k, v in batch.items() if k != 'labels'}\n",
    "        labels = batch['labels'].to(model.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "    elapsed_time = time.time() - start_time\n",
    "    return correct / total, elapsed_time\n",
    "\n",
    "train_accuracy, train_time = evaluate_model(model, train_dataset)\n",
    "val_accuracy, val_time = evaluate_model(model, val_dataset)\n",
    "test_accuracy, test_time = evaluate_model(model, test_dataset)\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy}, Time: {train_time}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy}, Time: {val_time}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}, Time: {test_time}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opencalm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
